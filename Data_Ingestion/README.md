# Data Ingestion Pipeline into Data Lake

This document explains the design and steps of our data ingestion pipeline, including how data moves across the **three zones** of the Data Lake.

---

## üåê Data Lake Location
`https://console.cloud.google.com/storage/browser/bigdata-ai-datalake?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&project=elegant-verbena-471612-d8`

---

## üèóÔ∏è Zones Design

### 1Ô∏è‚É£ Raw Zone
- **Purpose**: Store raw data exactly as ingested, without modifications.  
- **Sources**: GitHub repository (CSV & RAR files).  
- **Process**: Data is ingested daily using **Airflow DAG** (`data_ingestion_github_to_gcs`).  
- **Files**:
  - `Cust-churn.csv`
  - `E-commerce_Website_Logs.csv`
  - `consumer_complaints.rar`
  - `reviews.rar`

---

### 2Ô∏è‚É£ Curated Zone
- **Purpose**: Clean, standardize, and prepare data for downstream usage.  
- **Processing**:  
  - **Batch processing** for structured files (CSV, RAR extracted data) using **Spark**.  
  - **Streaming processing** for clickstream logs:
    - Ingested through **Kafka**.  
    - Processed in real-time with **Apache Flink**.  
- **Output**: Curated data is stored in partitioned format (e.g., Parquet/ORC) for optimized analytics.

---

### 3Ô∏è‚É£ Enriched Zone
- **Purpose**: Store enriched datasets ready for advanced analytics, ML, and visualization.  
- **Examples**:
  - Aggregated KPIs from churn and complaints data.  
  - Real-time dashboards on website clickstream behavior.  
- **Tools**:
  - BI Tools (e.g., Looker Studio, Power BI, Tableau).  
  - Machine Learning pipelines (future extension).  

---

## ‚öôÔ∏è Pipeline Flow

1. **Ingestion**  
   Raw files are ingested from GitHub ‚Üí GCS **Raw Zone**.

2. **Processing**  
   - Batch jobs (Spark) clean and transform data ‚Üí **Curated Zone**.  
   - Streaming jobs (Kafka + Flink) process real-time clickstream logs ‚Üí **Curated Zone**.  

3. **Enrichment & Visualization**  
   Final datasets are stored in the **Enriched Zone** for reporting, dashboards, and ML.

---

## üìå Next Steps
- Automate batch processing with **Airflow operators**.  
- Set up Kafka + Flink streaming jobs for clickstream data.  
- Integrate visualization dashboards.  

---
